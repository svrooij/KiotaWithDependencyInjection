// <auto-generated/>
using Microsoft.Kiota.Abstractions.Serialization;
using System.Collections.Generic;
using System.IO;
using System.Linq;
using System;
namespace Spotify.Models {
    public class SegmentObject : IAdditionalDataHolder, IParsable 
    {
        /// <summary>Stores additional data not described in the OpenAPI description found when deserializing. Can be used for serialization as well.</summary>
        public IDictionary<string, object> AdditionalData { get; set; }
        /// <summary>The confidence, from 0.0 to 1.0, of the reliability of the segmentation. Segments of the song which are difficult to logically segment (e.g: noise) may correspond to low values in this field.</summary>
        public double? Confidence { get; set; }
        /// <summary>The duration (in seconds) of the segment.</summary>
        public double? Duration { get; set; }
        /// <summary>The offset loudness of the segment in decibels (dB). This value should be equivalent to the loudness_start of the following segment.</summary>
        public double? LoudnessEnd { get; set; }
        /// <summary>The peak loudness of the segment in decibels (dB). Combined with `loudness_start` and `loudness_max_time`, these components can be used to describe the &quot;attack&quot; of the segment.</summary>
        public double? LoudnessMax { get; set; }
        /// <summary>The segment-relative offset of the segment peak loudness in seconds. Combined with `loudness_start` and `loudness_max`, these components can be used to desctibe the &quot;attack&quot; of the segment.</summary>
        public double? LoudnessMaxTime { get; set; }
        /// <summary>The onset loudness of the segment in decibels (dB). Combined with `loudness_max` and `loudness_max_time`, these components can be used to describe the &quot;attack&quot; of the segment.</summary>
        public double? LoudnessStart { get; set; }
        /// <summary>Pitch content is given by a “chroma” vector, corresponding to the 12 pitch classes C, C#, D to B, with values ranging from 0 to 1 that describe the relative dominance of every pitch in the chromatic scale. For example a C Major chord would likely be represented by large values of C, E and G (i.e. classes 0, 4, and 7).Vectors are normalized to 1 by their strongest dimension, therefore noisy sounds are likely represented by values that are all close to 1, while pure tones are described by one value at 1 (the pitch) and others near 0.As can be seen below, the 12 vector indices are a combination of low-power spectrum values at their respective pitch frequencies.![pitch vector](https://developer.spotify.com/assets/audio/Pitch_vector.png)</summary>
#if NETSTANDARD2_1_OR_GREATER || NETCOREAPP3_1_OR_GREATER
#nullable enable
        public List<double?>? Pitches { get; set; }
#nullable restore
#else
        public List<double?> Pitches { get; set; }
#endif
        /// <summary>The starting point (in seconds) of the segment.</summary>
        public double? Start { get; set; }
        /// <summary>Timbre is the quality of a musical note or sound that distinguishes different types of musical instruments, or voices. It is a complex notion also referred to as sound color, texture, or tone quality, and is derived from the shape of a segment’s spectro-temporal surface, independently of pitch and loudness. The timbre feature is a vector that includes 12 unbounded values roughly centered around 0. Those values are high level abstractions of the spectral surface, ordered by degree of importance.For completeness however, the first dimension represents the average loudness of the segment; second emphasizes brightness; third is more closely correlated to the flatness of a sound; fourth to sounds with a stronger attack; etc. See an image below representing the 12 basis functions (i.e. template segments).![timbre basis functions](https://developer.spotify.com/assets/audio/Timbre_basis_functions.png)The actual timbre of the segment is best described as a linear combination of these 12 basis functions weighted by the coefficient values: timbre = c1 x b1 + c2 x b2 + ... + c12 x b12, where c1 to c12 represent the 12 coefficients and b1 to b12 the 12 basis functions as displayed below. Timbre vectors are best used in comparison with each other.</summary>
#if NETSTANDARD2_1_OR_GREATER || NETCOREAPP3_1_OR_GREATER
#nullable enable
        public List<double?>? Timbre { get; set; }
#nullable restore
#else
        public List<double?> Timbre { get; set; }
#endif
        /// <summary>
        /// Instantiates a new <see cref="SegmentObject"/> and sets the default values.
        /// </summary>
        public SegmentObject()
        {
            AdditionalData = new Dictionary<string, object>();
        }
        /// <summary>
        /// Creates a new instance of the appropriate class based on discriminator value
        /// </summary>
        /// <returns>A <see cref="SegmentObject"/></returns>
        /// <param name="parseNode">The parse node to use to read the discriminator value and create the object</param>
        public static SegmentObject CreateFromDiscriminatorValue(IParseNode parseNode)
        {
            _ = parseNode ?? throw new ArgumentNullException(nameof(parseNode));
            return new SegmentObject();
        }
        /// <summary>
        /// The deserialization information for the current model
        /// </summary>
        /// <returns>A IDictionary&lt;string, Action&lt;IParseNode&gt;&gt;</returns>
        public virtual IDictionary<string, Action<IParseNode>> GetFieldDeserializers()
        {
            return new Dictionary<string, Action<IParseNode>>
            {
                {"confidence", n => { Confidence = n.GetDoubleValue(); } },
                {"duration", n => { Duration = n.GetDoubleValue(); } },
                {"loudness_end", n => { LoudnessEnd = n.GetDoubleValue(); } },
                {"loudness_max", n => { LoudnessMax = n.GetDoubleValue(); } },
                {"loudness_max_time", n => { LoudnessMaxTime = n.GetDoubleValue(); } },
                {"loudness_start", n => { LoudnessStart = n.GetDoubleValue(); } },
                {"pitches", n => { Pitches = n.GetCollectionOfPrimitiveValues<double?>()?.ToList(); } },
                {"start", n => { Start = n.GetDoubleValue(); } },
                {"timbre", n => { Timbre = n.GetCollectionOfPrimitiveValues<double?>()?.ToList(); } },
            };
        }
        /// <summary>
        /// Serializes information the current object
        /// </summary>
        /// <param name="writer">Serialization writer to use to serialize this model</param>
        public virtual void Serialize(ISerializationWriter writer)
        {
            _ = writer ?? throw new ArgumentNullException(nameof(writer));
            writer.WriteDoubleValue("confidence", Confidence);
            writer.WriteDoubleValue("duration", Duration);
            writer.WriteDoubleValue("loudness_end", LoudnessEnd);
            writer.WriteDoubleValue("loudness_max", LoudnessMax);
            writer.WriteDoubleValue("loudness_max_time", LoudnessMaxTime);
            writer.WriteDoubleValue("loudness_start", LoudnessStart);
            writer.WriteCollectionOfPrimitiveValues<double?>("pitches", Pitches);
            writer.WriteDoubleValue("start", Start);
            writer.WriteCollectionOfPrimitiveValues<double?>("timbre", Timbre);
            writer.WriteAdditionalData(AdditionalData);
        }
    }
}
